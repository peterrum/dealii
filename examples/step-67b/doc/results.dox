<h1>Results</h1>

Running the program with the default settings on a machine with 40 processes
produces the following output:

@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 27.648.000 ( = 5 [vars] x 25.600 [cells] x 216 [dofs/cell/var] )
Time step size: 0.000295952, minimal h: 0.0075, initial transport scaling: 0.00441179
Time:       0, dt:   0.0003, norm rho:  5.385e-16, rho * u:  1.916e-16, energy: 1.547e-15
+--------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed         |     17.52s    10 |     17.52s |     17.52s    11 |
|                                      |                  |                               |
| Section                  | no. calls |   min time  rank |   avg time |   max time  rank |
+--------------------------------------+------------------+------------+------------------+
| compute errors           |         1 |  0.009594s    16 |  0.009705s |  0.009819s     8 |
| compute transport speed  |        22 |    0.1366s     0 |    0.1367s |    0.1368s    18 |
| output                   |         1 |     1.233s     0 |     1.233s |     1.233s    32 |
| rk time stepping total   |       100 |     8.746s    35 |     8.746s |     8.746s     0 |
| rk_stage - integrals L_h |       500 |     8.742s    36 |     8.742s |     8.743s     2 |
+--------------------------------------+------------------+------------+------------------+
@endcode

As a reference, the results of step-67 using FCL is:

@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 27.648.000 ( = 5 [vars] x 25.600 [cells] x 216 [dofs/cell/var] )
Time step size: 0.000295952, minimal h: 0.0075, initial transport scaling: 0.00441179
Time:       0, dt:   0.0003, norm rho:  5.385e-16, rho * u:  1.916e-16, energy: 1.547e-15
+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              |     13.33s     0 |     13.34s |     13.35s    34 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |         1 |  0.007977s    10 |  0.008053s |  0.008161s    30 |
| compute transport speed       |        22 |    0.1228s    34 |    0.2227s |    0.3845s     0 |
| output                        |         1 |     1.255s     3 |     1.257s |     1.259s    27 |
| rk time stepping total        |       100 |     11.15s     0 |     11.32s |     11.42s    34 |
| rk_stage - integrals L_h      |       500 |     8.719s    10 |     8.932s |     9.196s     0 |
| rk_stage - inv mass + vec upd |       500 |     1.944s     0 |     2.377s |      2.55s    10 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

By the modifications shown in this tutorial, we were able to achieve a speedup of
27% for the Runge-Kutta stages.

<h3>Possibilities for extensions</h3>

<h4>Extension to the compressible Navier-Stokes equation</h4>

The solver presented in this tutorial program can also be extended to the
compressible Navierâ€“Stokes equations by adding viscous terms, as also
suggested in step-67. To keep as much of the performance obtained here despite
the additional cost of elliptic terms, e.g. via an interior penalty method, that
tutorial suggested to switch the basis from FE_DGQ to FE_DGQHermite like in
the step-59 tutorial program. The reasoning behind this switch is that in the
case of FE_DGQ all values of neighboring cells (i.e., $k+1$ layers) are needed,
whilst in the case of FE_DGQHermite only 2 layers, making the latter
significantly more suitable for higher degrees. The additional layers have to be
on the one hand loaded from main memory while flux computation while one the
other hand have to be communicated. Using the shared memory capabilities
introduced in this tutorial, the second point can be eliminated on a single
compute node or its influence can be reduced in a hybrid context.

<h4>Block Gauss-Seidl like preconditioners</h4>

Cell-centric loops could be used to create block Gauss-Seidl preconditioners. These
type of preconditioners use, in contrast to Jacobi type precondtioners, during
flux computation already updated values from neighboring cells. The following
pseudo-code visualizes how this could in principal be achieved:

@code
// vector monitor if cells have been updated or not
Vector<Number> visit_flags; // TODO: we need a function for setup

// element centric loop with a modified kernel
data.template loop_cell_centric<VectorType, VectorType>(
  [&](const auto &data, auto &dst, const auto &src, const auto cell_range) {

    for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)
      {
        // cell integral as usual (not shown)

        for (unsigned int face = 0; face < GeometryInfo<dim>::faces_per_cell; ++face)
          {
            const auto boundary_id = data.get_faces_by_cells_boundary_id(cell, face)[0];

            if (boundary_id == numbers::internal_face_boundary_id)
              {
                phi_p.reinit(cell, face);

                const auto flags = phi_p.read_cell_data(visit_flags);
                const auto all_neighbors_have_been_updated =
                  std::min(flags.begin(),
                           flags().begin() + data.n_active_entries_per_cell_batch(cell) == 1;

                if(all_neighbors_have_been_updated)
                  phi_p.gather_evaluate(dst, EvaluationFlags::values);
                else
                  phi_p.gather_evaluate(src, EvaluationFlags::values);

                // continue as usual (not shown)
              }
            else
              {
                // boundary integral as usual (not shown)
              }
          }

        // continue as usual (not shown)

        // make cells as updated
        phi.set_cell_data(visit_flags, VectorizedArrayType(1.0));
      }
  },
  dst,
  src,
  true,
  MatrixFree<dim, Number, VectorizedArrayType>::DataAccessOnFaces::values);
@endcode

For this purpose, one can exploited the cell-data vector capabilities of
MatrixFree.
