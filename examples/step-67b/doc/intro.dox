
<br>

<i>
This program was contributed by Martin Kronbichler, Peter Munch, and David
Schneider. Many of the features shown here have been added to deal.II during
and for the development of the deal.II-based, efficient, matrix-free
finite-element library for high-dimensional partial differential equations
hyper.deal (see https://github.com/hyperdeal/hyperdeal). For more details and
for the application of these features in slightly different contexts (high-
dimensional advection equation and Vlasov-Poisson equations) see the release
paper "P. Munch, K. Kormann, M. Kronbichler, hyper.deal: An efficient,
matrix-free finite-element library for high-dimensional partial differential
equations, 2020".

This work was partly supported by the German Research Foundation (DFG) through
the project "High-order discontinuous Galerkin for the exa-scale" (ExaDG)
within the priority program "Software for Exascale Computing" (SPPEXA) and
by the Bavarian government through the project "High-order matrix-free finite
element implementations with hybrid parallelization and improved data locality"
within the KONWIHR program.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program solves the Euler equations of fluid dynamics using an
explicit time integrator with the matrix-free framework applied to a
high-order discontinuous Galerkin discretization in space. The numerical
approach used here is identical to that used in step-67, however, we use
different advanced MatrixFree features to reach even higher throughput.

The two main features of this tutorial are:
- the usage of shared memory features from MPI-3.0 and
- the usage of cell-centric loops which allows to write to the global vector only
  once.

Further topics we discuss in this tutorial are the usage and benefits of the
template argument VectorizedArrayType (instead of simply using
VectorizedArray<Number>), as well as, the possibility to pass lambdas to
MatrixFree loops.

For details on the numerics, we refer to the documentation of step-67. We
concentrate here only on the differences.

<h3>Shared-memory and hybrid parallelization with MPI-3.0</h3>

There exist many shared-memory libraries that are based on threads, e.g., TBB,
OpenMP, or TaskFlow. Integrating such libraries into existing MPI programs
allows to use shared memory, however, comes with an overhead for the programmer,
since all parallelizable code sections have to be found and transformed
according to the library used, including the difficulty when some third-party
numerical library like an iterative solver package only relies on MPI.

TODO: Motivation for MPI-3.0

TODO: commands

TODO: comment: that these comments are part of L:d:V

TODO: what does MatrixFree do

TODO: how do you have to configure MatrixFree


<h3>Cell-centric loops</h3>

``Cell-centric loops'' process a cell and in direct succession processing all its
2d faces (i.e., visiting all faces twice). Their benefit has become clear for
modern CPU processor architecture in the literature [25], although this kind of
loop implies that fluxes have to be computed twice (for each side of an interior
face). The reasons for the advantage of ECL are twofold:
- On the one hand, entries in the solution vector are written exactly only once
  back to main memory in the case of ECL, while in the case of FCL at least once
  despite of cache-efficient scheduling of cell and face loops-due to cache
  capacity misses.
- On the other hand, since each entry of the solution vector is accessed only
  exactly once, no synchronization between threads is needed while accessing
  the solution vector in the case of ECL. This absence of race conditions during
  writing into the destination vector makes ECL particularly suitable for
  shared-memory parallelization.

One should also note that although fluxes are computed twice in the case of ECL,
this does not automatically translate into doubling of the computation, since
values already interpolated to the cell quadrature points can be interpolated
to a face with a single 1D sweep.

For cell-centric loop implementations the function `MatrixFree::loop_cell_centric()`
can be used, to which a user can pass a function that should be performed on
each cell.

The functions provided to a `MatrixFree::loop()`:

TODO

might in principal look like this in the context of cell-centric loops:

TODO

It should be noted that `FEFaceEvaluation` are initialized now with to numbers,
the cell number and the local face number (<2*dim). The given example only
highlights how to transform face-centric loops to cell-centric loops and the
result is by no mean efficient, since data is read and written multiple times
from and to the global vector, as well as, computations are performed
redundantly.Below, we will discuss advanced techniques that target these issues.

To be able to use `MatrixFree::loop_cell_centric()` following flags have to be
enabled of `MatirxFree::AdditionalData`:

TODO

Currently, cell-centric loops in deal.II only work for uniformly refined meshes.


